{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start..\n"
     ]
    }
   ],
   "source": [
    "# 3장\n",
    "print('start..')\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# 문자열에서 soup을 생성한다.\n",
    "soup1 = BeautifulSoup(\"<HTML><HEAD><header></HEAD><body></HTML>\")\n",
    "\n",
    "# 로컬 파일에서 soup을 생성한다. 내려받은 myDoc.html 파일을 사용한다.\n",
    "soup2 = BeautifulSoup(open(\"myDoc.html\"))\n",
    "\n",
    "# 웹 문서에서 soup을 생성한다.\n",
    "# urlopen()이 “http://“를 자동으로 추가해주지 않는다는 것을 기억하자!\n",
    "soup3 = BeautifulSoup(urlopen(\"http://www.networksciencelab.com/\"))\n",
    "#---------------------------------------\n",
    "\n",
    "htmlString = ''' <HTML>\n",
    "  <HEAD><TITLE>My document</TITLE></HEAD>\n",
    "  <BODY>Main text.</BODY></HTML>\n",
    "'''\n",
    "soup = BeautifulSoup(htmlString)\n",
    "mystr=soup.get_text()\n",
    "# print(mystr)  # Uchang\n",
    "#---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.661016949152543 43.27973735299687\n"
     ]
    }
   ],
   "source": [
    "with urlopen(\"http://www.networksciencelab.com/\") as doc: \n",
    "  soup = BeautifulSoup(doc)\n",
    "\n",
    "# print(soup) # Uchang\n",
    "links = [(link.string, link[\"href\"])\n",
    "    for link in soup.find_all(\"a\")\n",
    "    if link.has_attr(\"href\")]\n",
    "links\n",
    "#---------------------------------------\n",
    "\n",
    "import csv\n",
    "with open(\"Demographic_Statistics_By_Zip_Code.csv\", newline='') as infile:\n",
    "  data = list(csv.reader(infile))\n",
    "#---------------------------------------\n",
    "\n",
    "countParticipantsIndex = data[0].index(\"COUNT PARTICIPANTS\")\n",
    "countParticipantsIndex\n",
    "#---------------------------------------\n",
    "\n",
    "import statistics\n",
    "countParticipants = [int(row[countParticipantsIndex]) for row in data[1:]]\n",
    "print(statistics.mean(countParticipants), statistics.stdev(countParticipants))\n",
    "#---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an informal term for a youth or man', 'informal term for a man']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "\n",
    "wn = nltk.corpus.wordnet # 코퍼스 리더(reader)\n",
    "wn.synsets(\"cat\")\n",
    "#---------------------------------------\n",
    "\n",
    "wn.synset(\"cat.n.01\").hypernyms() \n",
    "wn.synset(\"cat.n.01\").hyponyms()\n",
    "#---------------------------------------\n",
    "\n",
    "x = wn.synset(\"cat.n.01\")\n",
    "y = wn.synset(\"lynx.n.01\")\n",
    "x.path_similarity(y)\n",
    "#---------------------------------------\n",
    "\n",
    "[simxy.definition() for simxy in max(\n",
    "  (x.path_similarity(y), x, y)\n",
    "  for x in wn.synsets('cat')\n",
    "  for y in wn.synsets('dog')\n",
    "  if x.path_similarity(y) # synset들이 서로 관련 있는지 확인한다.\n",
    ")[1:]]\n",
    "#---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beautiful', 'JJ'), ('world', 'NN')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer \n",
    "word_punct = WordPunctTokenizer()\n",
    "text = \"}Help! :))) :[ ..... :D{\" \n",
    "word_punct.tokenize(text)\n",
    "#---------------------------------------\n",
    "\n",
    "nltk.word_tokenize(text)\n",
    "#---------------------------------------\n",
    "\n",
    "pstemmer = nltk.PorterStemmer() \n",
    "pstemmer.stem(\"wonderful\")\n",
    "#---------------------------------------\n",
    "\n",
    "lstemmer = nltk.LancasterStemmer()\n",
    "lstemmer.stem(\"wonderful\") \n",
    "#---------------------------------------\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"wonderful\")\n",
    "#---------------------------------------\n",
    "\n",
    "nltk.pos_tag([\"beautiful\", \"world\"])\n",
    "#---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['New', 'Document', 'document.location', '=', \"'menu.jsp\", \"'\", ';']\n",
      "[('h', 1), ('e', 1), ('l', 1), ('p', 1), ('d', 1)]\n",
      "end...\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import LancasterStemmer\n",
    "\n",
    "# 형태소 분류기를 생성한다.\n",
    "ls = nltk.LancasterStemmer()\n",
    "\n",
    "# 파일을 읽고 soup을 만든다. \n",
    "with open(\"index.html\") as infile:\n",
    "  soup = BeautifulSoup(infile)\n",
    "\n",
    "# 텍스트를 추출하고 토큰화한다.\n",
    "words = nltk.word_tokenize(soup.text)\n",
    "print(words) # Uchang\n",
    "# 단어를 소문자로 변환한다.\n",
    "words = [w.lower() for w in words]\n",
    "\n",
    "# 불용어를 제거하고 단어의 형태소를 추출한다.\n",
    "words = [ls.stem(w) for w in text if w not in stopwords.words(\"english\") and w.isalnum()]\n",
    "\n",
    "# 가장 빈번하게 등장한 단어 10개를 추출한다. \n",
    "freqs = Counter(words) \n",
    "print(freqs.most_common(10))\n",
    "print('end...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
